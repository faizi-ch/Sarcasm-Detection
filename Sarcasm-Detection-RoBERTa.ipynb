{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/ishfaqm0/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed=None):\n",
    "    \"\"\"\n",
    "    Using random seed for numpy and torch\n",
    "    \"\"\"\n",
    "    if random_seed is None:\n",
    "        random_seed = 13\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm_df = pd.read_csv(\"data/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need comment & label columns\n",
    "# So, let's remove others.\n",
    "sarcasm_df.drop(\n",
    "    [\n",
    "        \"author\",\n",
    "        \"subreddit\",\n",
    "        \"score\",\n",
    "        \"ups\",\n",
    "        \"downs\",\n",
    "        \"date\",\n",
    "        \"created_utc\",\n",
    "        \"parent_comment\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "# remove empty rows\n",
    "sarcasm_df.dropna(inplace=True)\n",
    "\n",
    "# Some comments are missing, so we drop the corresponding rows.\n",
    "sarcasm_df.dropna(subset=[\"comment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    505403\n",
       "1    505368\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 10.461467533199905\n",
      "Maximum length: 2222\n",
      "Minimum length: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lengths of comments\n",
    "comment_lengths = [len(comment.split()) for comment in sarcasm_df[\"comment\"]]\n",
    "\n",
    "# Calculate the mean, maximum, and minimum lengths\n",
    "mean_length = sum(comment_lengths) / len(comment_lengths)\n",
    "max_length = max(comment_lengths)\n",
    "min_length = min(comment_lengths)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean length:\", mean_length)\n",
    "print(\"Maximum length:\", max_length)\n",
    "print(\"Minimum length:\", min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to keep only comments with length <= 50\n",
    "mask = [length <= 50 for length in comment_lengths]\n",
    "sarcasm_df = sarcasm_df[mask]\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "sarcasm_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 10.265359705412772\n",
      "Maximum length: 50\n",
      "Minimum length: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lengths of comments\n",
    "comment_lengths = [len(comment.split()) for comment in sarcasm_df[\"comment\"]]\n",
    "\n",
    "# Calculate the mean, maximum, and minimum lengths\n",
    "mean_length = sum(comment_lengths) / len(comment_lengths)\n",
    "max_length = max(comment_lengths)\n",
    "min_length = min(comment_lengths)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean length:\", mean_length)\n",
    "print(\"Maximum length:\", max_length)\n",
    "print(\"Minimum length:\", min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    504617\n",
       "0    503166\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sarcasm_df[\"comment\"], sarcasm_df[\"label\"], test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "    list(X_train),\n",
    "    list(X_test),\n",
    "    list(y_train),\n",
    "    list(y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARCDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        texts = X\n",
    "\n",
    "        texts = [preprocess(text) for text in tqdm(texts, desc=\"Preprocessing\")]\n",
    "\n",
    "        self._print_random_samples(texts)\n",
    "\n",
    "        self.texts = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=150,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            for text in tqdm(texts, desc=\"Tokenizing\")\n",
    "        ]\n",
    "\n",
    "        self.labels = y\n",
    "\n",
    "    def _print_random_samples(self, texts):\n",
    "        random_entries = np.random.randint(0, len(texts), 5)\n",
    "\n",
    "        for i in random_entries:\n",
    "            print(f\"Entry {i}: {texts[i]}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        label = -1\n",
    "        if hasattr(self, \"labels\"):\n",
    "            label = self.labels[idx]\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|          | 0/1007783 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 1007783/1007783 [00:13<00:00, 72319.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 121958: [CLS] I read this as man or no man ? [SEP]\n",
      "Entry 671155: [CLS] Wait but I thought Islamists were responsible for basically every war right ? [SEP]\n",
      "Entry 131932: [CLS] Serious question is there any other kind of meat for Christmas ? [SEP]\n",
      "Entry 365838: [CLS] Watch your tone you sexist ! [SEP]\n",
      "Entry 259178: [CLS] My parents figured I was going to do it anyway so they would rather just know about it and make sure I was being responsible [SEP]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 1007783/1007783 [03:21<00:00, 4990.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = SARCDataset(sarcasm_df[\"comment\"], sarcasm_df[\"label\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset using pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"preprocessed_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset using pickle\n",
    "with open(\"preprocessed_dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 755838/755838 [00:10<00:00, 72482.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 521430: [CLS] overrated futbol underrated Hockey [SEP]\n",
      "Entry 87498: [CLS] Certainly the last thing Labor wants the press gallery talking about are The NBN National Disability Care Gonkski reforms [SEP]\n",
      "Entry 175203: [CLS] Definitely bonking [SEP]\n",
      "Entry 191335: [CLS] where did you get that little Majoras mask figure ? [SEP]\n",
      "Entry 278167: [CLS] And the fact that Putin was right and Obama was wrong on Syria is secondary of course [SEP]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 755838/755838 [02:26<00:00, 5148.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sarc = SARCDataset(X_train, y_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 251947/251947 [00:03<00:00, 72583.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 119879: [CLS] The idea that players get paid based solely on production is nonsense [SEP]\n",
      "Entry 110268: [CLS] Than [SEP]\n",
      "Entry 207892: [CLS] To be fair everyone knows that type of people that would buy a MacBook Pro are the kind of people that would only use it for Facebook and to show off that Apple logo [SEP]\n",
      "Entry 54886: [CLS] we have solved the mystery of who Im Uzi is boys ! [SEP]\n",
      "Entry 137337: [CLS] I guess we are all taking it a bit hard during the shutdown [SEP]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 251947/251947 [00:49<00:00, 5079.96it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sarc = SARCDataset(X_test, y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_sarc, batch_size=128, shuffle=True, num_workers=16)\n",
    "val_dataloader = DataLoader(test_sarc, batch_size=128, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARCClassifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(SARCClassifier, self).__init__()\n",
    "\n",
    "        self.bert = base_model\n",
    "        self.fc1 = nn.Linear(768, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0][\n",
    "            :, 0\n",
    "        ]\n",
    "        x = self.fc1(bert_out)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping_threshold_count = 0\n",
    "    EARLY_STOPPING = 3\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    model_metrics = {}\n",
    "    model_metrics[\"train_accuracy\"] = []\n",
    "    model_metrics[\"val_accuracy\"] = []\n",
    "    model_metrics[\"train_loss\"] = []\n",
    "    model_metrics[\"val_loss\"] = []\n",
    "    model_metrics[\"f1\"] = []\n",
    "    model_metrics[\"val_f1\"] = []\n",
    "    model_metrics[\"auc\"] = []\n",
    "    model_metrics[\"val_auc\"] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        total_f1_train = 0\n",
    "        total_auc_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            attention_mask = train_input[\"attention_mask\"].to(device)\n",
    "            input_ids = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = criterion(output, train_label.float().unsqueeze(1))\n",
    "\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            acc = ((output >= 0.5).int() == train_label.unsqueeze(1)).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            out_preds = output.cpu().detach().numpy().flatten()\n",
    "            targets = train_label.cpu().detach().numpy().flatten()\n",
    "            try:\n",
    "                auc_score = roc_auc_score(targets, out_preds)\n",
    "            except:\n",
    "                auc_score = 1\n",
    "            total_auc_train += auc_score\n",
    "\n",
    "            out_preds[out_preds < 0.5] = 0\n",
    "            out_preds[out_preds >= 0.5] = 1\n",
    "            f1_score_ = f1_score(targets, out_preds)\n",
    "            total_f1_train += f1_score_\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            total_f1_val = 0\n",
    "            total_auc_val = 0\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for val_input, val_label in tqdm(val_dataloader):\n",
    "                attention_mask = val_input[\"attention_mask\"].to(device)\n",
    "                input_ids = val_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "\n",
    "                output = model(input_ids, attention_mask)\n",
    "\n",
    "                loss = criterion(output, val_label.float().unsqueeze(1))\n",
    "\n",
    "                total_loss_val += loss.item()\n",
    "\n",
    "                acc = ((output >= 0.5).int() == val_label.unsqueeze(1)).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "                out_preds = output.cpu().detach().numpy().flatten()\n",
    "                targets = val_label.cpu().detach().numpy().flatten()\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(targets, out_preds)\n",
    "                except:\n",
    "                    auc_score = 1.0\n",
    "                total_auc_val += auc_score\n",
    "\n",
    "                out_preds[out_preds < 0.5] = 0\n",
    "                out_preds[out_preds >= 0.5] = 1\n",
    "                f1_score_ = f1_score(targets, out_preds)\n",
    "                total_f1_val += f1_score_\n",
    "\n",
    "            print(\n",
    "                f\"Epochs: {epoch + 1} \"\n",
    "                f\"| Train Loss: {total_loss_train / len(train_dataloader): .3f} \"\n",
    "                f\"| Train Accuracy: {total_acc_train / (len(train_dataloader.dataset)): .3f} \"\n",
    "                f\"| Val Loss: {total_loss_val / len(val_dataloader): .3f} \"\n",
    "                f\"| Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}\"\n",
    "            )\n",
    "            model_metrics[\"train_accuracy\"].append(\n",
    "                total_acc_train / (len(train_dataloader.dataset))\n",
    "            )\n",
    "            model_metrics[\"val_accuracy\"].append(\n",
    "                total_acc_val / len(val_dataloader.dataset)\n",
    "            )\n",
    "            model_metrics[\"train_loss\"].append(total_loss_train / len(train_dataloader))\n",
    "            model_metrics[\"val_loss\"].append(total_loss_val / len(val_dataloader))\n",
    "            model_metrics[\"f1\"].append(total_f1_train / len(train_dataloader))\n",
    "            model_metrics[\"val_f1\"].append(total_f1_val / len(val_dataloader))\n",
    "            model_metrics[\"auc\"].append(total_auc_train / len(train_dataloader))\n",
    "            model_metrics[\"val_auc\"].append(total_auc_val / len(val_dataloader))\n",
    "\n",
    "            print(model_metrics)\n",
    "\n",
    "            if best_val_loss > total_loss_val:\n",
    "                best_val_loss = total_loss_val\n",
    "                torch.save(model, f\"best_model.pt\")\n",
    "                print(\"Saved model\")\n",
    "                early_stopping_threshold_count = 0\n",
    "            else:\n",
    "                early_stopping_threshold_count += 1\n",
    "\n",
    "            if early_stopping_threshold_count >= EARLY_STOPPING:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m SARCClassifier(model)\n\u001b[0;32m----> 3\u001b[0m dist\u001b[39m.\u001b[39;49minit_process_group(backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mnccl\u001b[39;49m\u001b[39m'\u001b[39;49m, init_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39menv://\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39mDistributedDataParallel(model)\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:900\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[39mif\u001b[39;00m store \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    897\u001b[0m     rendezvous_iterator \u001b[39m=\u001b[39m rendezvous(\n\u001b[1;32m    898\u001b[0m         init_method, rank, world_size, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m    899\u001b[0m     )\n\u001b[0;32m--> 900\u001b[0m     store, rank, world_size \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(rendezvous_iterator)\n\u001b[1;32m    901\u001b[0m     store\u001b[39m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m    903\u001b[0m     \u001b[39m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[39m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/rendezvous.py:235\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(query_dict[\u001b[39m\"\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(_get_env_or_raise(\u001b[39m\"\u001b[39;49m\u001b[39mRANK\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mworld_size\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m query_dict:\n\u001b[1;32m    238\u001b[0m     world_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(query_dict[\u001b[39m\"\u001b[39m\u001b[39mworld_size\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/rendezvous.py:220\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    218\u001b[0m env_val \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(env_var, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m env_val:\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mraise\u001b[39;00m _env_error(env_var)\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[39mreturn\u001b[39;00m env_val\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "model = SARCClassifier(model)\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "model = nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[39m=\u001b[39m train(model, train_dataloader, val_dataloader, learning_rate, epochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = train(model, train_dataloader, val_dataloader, learning_rate, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
